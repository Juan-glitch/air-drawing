{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are used over point sequence to predict whether the user wanted to have the pencil up or pencil down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import cv2\n",
    "from os import listdir\n",
    "from contextlib import ExitStack\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrawingsDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, folder=\"../../data/processed_labeled/\"):\n",
    "        self.folder = folder\n",
    "        self.ds = []\n",
    "        \n",
    "        for f in listdir(self.folder):\n",
    "            self.ds.append(self.load_file(f))\n",
    "        \n",
    "        self.compute_mean()\n",
    "        self.compute_std()\n",
    "        \n",
    "        for f in self.ds:\n",
    "            f['input'] = (f['input'] - self.mean)/self.std\n",
    "        \n",
    "    def compute_mean(self):\n",
    "        self.mean = np.zeros(6)\n",
    "        tot = 0\n",
    "        self.y_mean = 0\n",
    "        for f in self.ds:\n",
    "            x = f['input']\n",
    "            self.mean += np.sum(x,axis=0)\n",
    "            self.y_mean += np.sum(f['output'])\n",
    "            tot += x.shape[0]\n",
    "        self.mean /= tot\n",
    "        self.y_mean /= tot\n",
    "        \n",
    "        \n",
    "    def compute_std(self):\n",
    "        variance = np.zeros(6)\n",
    "        tot = 0\n",
    "        for f in self.ds:\n",
    "            x = f['input'] - self.mean\n",
    "            x = np.square(x)\n",
    "            variance += np.sum(x,axis=0)\n",
    "            tot += x.shape[0]\n",
    "        variance /= tot\n",
    "        self.std = np.sqrt(variance)\n",
    "        \n",
    "    def load_file(self,f):\n",
    "        df = pd.read_csv(self.folder+f,index_col=0)\n",
    "        raw_input = df[['x','y']].to_numpy().astype(np.int)\n",
    "        inputs = df[['vx','vy','v','ax','ay','a']].to_numpy().astype(np.double)\n",
    "        ouput = df[['label']].to_numpy().astype(np.double)\n",
    "        ttf = df[['ttf']].to_numpy().astype(np.double)\n",
    "        return {\n",
    "            'raw_input' : raw_input,\n",
    "            'input' : inputs,\n",
    "            'output' : ouput,\n",
    "            'ttf' : ttf,\n",
    "            'name': f\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.ds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.33941933 -0.22546204  9.42321543  0.02886129  0.1246964   6.15246857]\n",
      "0.581060116354234\n",
      "[11.0053177  10.57159029 12.01018687 10.97142049  9.97192017 13.48980253]\n",
      "(295, 6)\n",
      "(295, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = DrawingsDS()\n",
    "print(dataset.mean)\n",
    "print(dataset.y_mean)\n",
    "print(dataset.std)\n",
    "print(dataset[0]['input'].shape)\n",
    "print(dataset[0]['output'].shape)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,(35,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(sample,pred):\n",
    "    sample_output = np.squeeze(pred)\n",
    "    pts = sample['raw_input'][sample_output == True]\n",
    "\n",
    "    img = np.zeros((720,1280), dtype=np.uint8)\n",
    "    img[pts.T[1],pts.T[0]]=255\n",
    "    img = cv2.flip(img, 1)\n",
    "\n",
    "    cv2.imshow('frame', img)\n",
    "    key = cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self,input_size=6,output_size=1,hidden_size=64,num_layers=3):\n",
    "        super(RNN, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.1\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features = hidden_size,\n",
    "            out_features = output_size\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output, _ = self.rnn(x)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "    \n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self,input_size=6,output_size=1,hidden_size_in=64,hidden_size_out=32,num_layers=2):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = torch.nn.GRU(\n",
    "            input_size = hidden_size_in,\n",
    "            hidden_size = hidden_size_out,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features = input_size,\n",
    "            out_features = hidden_size_in\n",
    "        )\n",
    "        self.fc2 = torch.nn.Linear(\n",
    "            in_features = hidden_size_in,\n",
    "            out_features = hidden_size_in\n",
    "        )\n",
    "        self.fc3 = torch.nn.Linear(\n",
    "            in_features = hidden_size_out,\n",
    "            out_features = hidden_size_out\n",
    "        )\n",
    "        self.fc4 = torch.nn.Linear(\n",
    "            in_features = hidden_size_out,\n",
    "            out_features = output_size\n",
    "        )\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = self.fc2(self.relu(self.fc1(x)))\n",
    "        output, _ = self.lstm(output)\n",
    "        output = self.fc4(self.relu(self.fc3(output)))\n",
    "        return output\n",
    "    \n",
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self,input_size=6,output_size=1,hidden_size_in=64,hidden_size_out=32,num_layers=2):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size = hidden_size_in,\n",
    "            hidden_size = hidden_size_out,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True,\n",
    "            dropout = 0.3\n",
    "        )\n",
    "        self.fc1 = torch.nn.Linear(\n",
    "            in_features = input_size,\n",
    "            out_features = hidden_size_in\n",
    "        )\n",
    "        self.fc2 = torch.nn.Linear(\n",
    "            in_features = hidden_size_out,\n",
    "            out_features = output_size\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output = self.fc1(x)\n",
    "        output, _ = self.gru(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "      \n",
    "class WeibullGRU(torch.nn.Module):\n",
    "    def __init__(self,input_size=6,output_size=3,hidden_size=64,num_layers=3):\n",
    "        super(WeibullGRU, self).__init__()\n",
    "        self.gru = torch.nn.GRU(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,\n",
    "            num_layers = num_layers,\n",
    "            batch_first = True\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(\n",
    "            in_features = hidden_size,\n",
    "            out_features = output_size\n",
    "        )\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        output, _ = self.gru(x)\n",
    "        output = self.fc(output)\n",
    "        return torch.squeeze(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 301, 6])\n",
      "torch.Size([301, 1])\n",
      "torch.Size([1, 301, 1])\n",
      "torch.Size([1, 301, 1])\n",
      "torch.Size([301, 3])\n",
      "torch.Size([1, 301, 1])\n"
     ]
    }
   ],
   "source": [
    "model1 = LSTM().double()\n",
    "model2 = GRU().double()\n",
    "model3 = WeibullGRU().double()\n",
    "model4 = RNN().double()\n",
    "sample = torch.tensor(train_set[0]['input']).unsqueeze(0)\n",
    "sample_truth = torch.tensor(train_set[0]['ttf'])\n",
    "print(sample.size())\n",
    "print(sample_truth.size())\n",
    "print(model1(sample).size())\n",
    "print(model2(sample).size())\n",
    "print(model3(sample).size())\n",
    "print(model4(sample).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "softplus = torch.nn.Softplus()\n",
    "def weibull_loss(pred,y):\n",
    "    epsilon = 1e-8\n",
    "    alpha = torch.exp(pred[:,1])\n",
    "    beta = softplus(pred[:,0])\n",
    "    ya = (y+epsilon)/alpha\n",
    "    loss = torch.log(beta+epsilon) + beta*torch.log(ya) - torch.log(y+epsilon) - torch.pow(ya,beta)\n",
    "    return -torch.mean(loss)\n",
    "\n",
    "def compute_ttf_from_weibull(pred):\n",
    "    # computes the mean of each weibull distribution to make prediction about the ttf (time to failure)\n",
    "    alpha = torch.exp(pred[:,1])\n",
    "    beta = softplus(pred[:,0])\n",
    "    mean = alpha*torch.exp(torch.lgamma(1.+1./beta))\n",
    "    return mean\n",
    "\n",
    "def visualize_weibull_pred(pred,truth):\n",
    "    plt.plot(40*compute_ttf_from_weibull(pred).detach().numpy())\n",
    "    plt.plot(truth.detach().numpy())\n",
    "\n",
    "# print(sample.shape)\n",
    "# print(compute_ttf_from_weibull(model3(sample)[:,1:]).shape)\n",
    "# visualize_weibull_pred(model3(sample)[:,1:],sample_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred,y):\n",
    "    pred_np = pred.squeeze().detach().numpy()\n",
    "    y_np = y.squeeze().detach().numpy()\n",
    "    accuracy = accuracy_score(y_np,pred_np)\n",
    "    precision = precision_score(y_np,pred_np,zero_division=0)\n",
    "    recall = recall_score(y_np,pred_np,zero_division=0)\n",
    "    f1 = f1_score(y_np,pred_np,zero_division=0)\n",
    "    return accuracy, precision, recall, f1\n",
    "    \n",
    "def epoch_weibull(loader,optimizer,model,loss,w_loss,iteration_type = 'train'):\n",
    "    if iteration_type == 'train':\n",
    "        MODEL.train()\n",
    "    if iteration_type == 'test':\n",
    "        MODEL.eval()\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        if iteration_type == 'test':\n",
    "            gs = stack.enter_context(torch.no_grad())\n",
    "        \n",
    "        metrics = {\n",
    "            'loss': 0.,\n",
    "            'accuracy': 0.,\n",
    "            'precision': 0.,\n",
    "            'recall': 0.,\n",
    "            'f1': 0.\n",
    "        }\n",
    "        \n",
    "        n = len(loader)\n",
    "        for sample in loader:\n",
    "            # make predictions\n",
    "            x = sample['input']\n",
    "            y = sample['output'].squeeze()\n",
    "            ttf = sample['ttf'].squeeze()\n",
    "            pred = MODEL(x)\n",
    "            # compute losses\n",
    "            l_bce = loss(pred[:,0],y)\n",
    "            l_weibull = w_loss(pred[:,1:],ttf)\n",
    "            l = l_bce + l_weibull/5.\n",
    "            # apply backprop\n",
    "            if iteration_type == 'train':\n",
    "                OPTIMIZER.zero_grad()\n",
    "                l.backward()\n",
    "                OPTIMIZER.step()\n",
    "                \n",
    "            acc, prec, rec, f1 = compute_metrics(torch.sigmoid(pred[:,0])>0.5,y)\n",
    "            metrics['loss'] += l.item()/n\n",
    "            metrics['accuracy'] += acc/n\n",
    "            metrics['precision'] += prec/n\n",
    "            metrics['recall'] += rec/n\n",
    "            metrics['f1'] += f1/n\n",
    "    return metrics\n",
    "\n",
    "def epoch(loader,optimizer,model,loss,iteration_type = 'train'):\n",
    "    if iteration_type == 'train':\n",
    "        MODEL.train()\n",
    "    if iteration_type == 'test':\n",
    "        MODEL.eval()\n",
    "\n",
    "    with ExitStack() as stack:\n",
    "        if iteration_type == 'test':\n",
    "            gs = stack.enter_context(torch.no_grad())\n",
    "        \n",
    "        metrics = {\n",
    "            'loss': 0.,\n",
    "            'accuracy': 0.,\n",
    "            'precision': 0.,\n",
    "            'recall': 0.,\n",
    "            'f1': 0.\n",
    "        }\n",
    "        \n",
    "        n = len(loader)\n",
    "        for sample in loader:\n",
    "            # make predictions\n",
    "            x = sample['input']\n",
    "            y = sample['output'].squeeze()\n",
    "            pred = MODEL(x).squeeze()\n",
    "            # compute losses\n",
    "            l = loss(pred,y)\n",
    "            # apply backprop\n",
    "            if iteration_type == 'train':\n",
    "                OPTIMIZER.zero_grad()\n",
    "                l.backward()\n",
    "                OPTIMIZER.step()\n",
    "                \n",
    "            acc, prec, rec, f1 = compute_metrics(torch.sigmoid(pred)>0.5,y)\n",
    "            metrics['loss'] += l.item()/n\n",
    "            metrics['accuracy'] += acc/n\n",
    "            metrics['precision'] += prec/n\n",
    "            metrics['recall'] += rec/n\n",
    "            metrics['f1'] += f1/n\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: lmagne (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">glowing-feather-197</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/lmagne/r-drawing\" target=\"_blank\">https://wandb.ai/lmagne/r-drawing</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/lmagne/r-drawing/runs/3bhcvkqf\" target=\"_blank\">https://wandb.ai/lmagne/r-drawing/runs/3bhcvkqf</a><br/>\n",
       "                Run data is saved locally in <code>/home/loic/Documents/Prog/Projets/air-drawing/python-stuff/deep-learning/wandb/run-20210902_224659-3bhcvkqf</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "    \"EPOCHS\" : 1000,\n",
    "    \"BATCH_SIZE\" : 1,\n",
    "    \"LEARNING_RATE\" : 3e-4,\n",
    "    \"NUM_WORKERS\" : 2,\n",
    "    \"PIN_MEMORY\" : True,\n",
    "    \"MODEL_HIDDEN_SIZE_IN\" : 128,\n",
    "    \"MODEL_HIDDEN_SIZE_OUT\" : 128,\n",
    "    \"MODEL_NUM_LAYERS\" : 2,\n",
    "    \"WEIGHT_DECAY\" : 0.,\n",
    "    \"SCHEDULER_GAMMA\" : 1.,\n",
    "    \"SEED\" : 23421467\n",
    "}\n",
    "log = True\n",
    "if log:\n",
    "    run = wandb.init(project=\"r-drawing\",config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "  4%|▍         | 39/1000 [06:50<2:48:31, 10.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-92d56ade5e93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"EPOCHS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLOSS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mLOSS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-2f359f4d6fa2>\u001b[0m in \u001b[0;36mepoch\u001b[0;34m(loader, optimizer, model, loss, iteration_type)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                 \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/wandb/wandb_torch.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_tensor_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_track\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hook_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%wandb\n",
    "\n",
    "torch.manual_seed(config[\"SEED\"])\n",
    "np.random.seed(config[\"SEED\"])\n",
    "\n",
    "dataset = DrawingsDS()\n",
    "train_set, test_set = torch.utils.data.random_split(dataset,(35,5))\n",
    "\n",
    "MODEL = LSTM(\n",
    "    hidden_size_in = config[\"MODEL_HIDDEN_SIZE_IN\"],\n",
    "    hidden_size_out = config[\"MODEL_HIDDEN_SIZE_OUT\"],\n",
    "    num_layers = config[\"MODEL_NUM_LAYERS\"]\n",
    ").double()\n",
    "\n",
    "LOSS = torch.nn.BCEWithLogitsLoss(pos_weight = torch.tensor([1./dataset.y_mean]))\n",
    "\n",
    "W_LOSS = weibull_loss\n",
    "\n",
    "OPTIMIZER = torch.optim.Adam(\n",
    "    MODEL.parameters(),\n",
    "    lr = config[\"LEARNING_RATE\"],\n",
    "    weight_decay = config[\"WEIGHT_DECAY\"]\n",
    ")\n",
    "\n",
    "'''\n",
    "OPTIMIZER = torch.optim.SGD(\n",
    "    MODEL.parameters(),\n",
    "    lr = config[\"LEARNING_RATE\"],\n",
    "    momentum = 0.9\n",
    ")\n",
    "'''\n",
    "\n",
    "'''\n",
    "SCHEDULER = torch.optim.lr_scheduler.StepLR(\n",
    "    OPTIMIZER,\n",
    "    step_size = 100,\n",
    "    gamma = config[\"SCHEDULER_GAMMA\"]\n",
    ")\n",
    "'''\n",
    "SCHEDULER = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    OPTIMIZER,\n",
    "    milestones=[250,500,750,1000],\n",
    "    gamma=0.1\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_set,\n",
    "    batch_size = config[\"BATCH_SIZE\"],\n",
    "    num_workers = config[\"NUM_WORKERS\"],\n",
    "    pin_memory = config[\"PIN_MEMORY\"],\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    batch_size = config[\"BATCH_SIZE\"],\n",
    "    num_workers = config[\"NUM_WORKERS\"],\n",
    "    pin_memory = config[\"PIN_MEMORY\"],\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "\n",
    "if log:\n",
    "    wandb.watch(MODEL)\n",
    "\n",
    "for k in tqdm(range(config[\"EPOCHS\"])):\n",
    "    train_metrics = epoch(train_loader,OPTIMIZER,MODEL,LOSS,'train')\n",
    "    test_metrics = epoch(test_loader,OPTIMIZER,MODEL,LOSS,'test')\n",
    "    if log:\n",
    "        wandb.log({\n",
    "            \"loss_train\" : train_metrics[\"loss\"],\n",
    "            \"loss_test\" : test_metrics[\"loss\"],\n",
    "            \"accuracy_train\" : train_metrics[\"accuracy\"],\n",
    "            \"accuracy_test\" : test_metrics[\"accuracy\"],\n",
    "            \"precision_train\" : train_metrics[\"precision\"],\n",
    "            \"precision_test\" : test_metrics[\"precision\"],\n",
    "            \"recall_train\" : train_metrics[\"recall\"],\n",
    "            \"recall_test\" : test_metrics[\"recall\"],\n",
    "            \"f1_train\" : train_metrics[\"f1\"],\n",
    "            \"f1_test\" : test_metrics[\"f1\"]\n",
    "        })\n",
    "    SCHEDULER.step()\n",
    "\n",
    "if log:\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_set[1]\n",
    "print(sample['name'])\n",
    "x = torch.tensor(sample['input']).unsqueeze(0)\n",
    "pred = (torch.sigmoid(MODEL(x)) > 0.5).detach().numpy()\n",
    "visualize(sample,pred)\n",
    "visualize(sample,sample['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
